---
title: "Exploring the Impact of AI Tools on Stack Overflow: Post-ChatGPT Trends"
author: "Sophie Xu"
date: "April 30, 2025"
output:
  html_document:
    highlight: kate
    theme: cosmo
---

```{r setup, message=FALSE, echo=FALSE, warning=FALSE}
# Scraping
library(rvest); library(xml2); library(base)

# Dataframe
library(dtplyr); library(dplyr); library(tidyverse); library(Matrix)
library(data.table); library(reshape2); library(jsonlite); library(kableExtra)

# Visualization
library(leaflet); library(wordcloud2); library(viridis); library(grid); library(gridExtra)
library(ggplot2); library(ggcorrplot); library(plotly); library(widgetframe)

# Model
#r='http://r-forge.r-project.org'
#install.packages('Pmisc', repos=r)
library(mgcv); library(Pmisc); library(glmmTMB); library(performance); library(pscl)
library(DirichletReg); library(betareg)

# Language
library(tm); library(tidytext); library(textdata); library(quanteda.textstats)
library(topicmodels); library(tokenizers); library(stringr); library(text2vec); library(umap)

# HPC
library(microbenchmark); library(parallel); library(doParallel)
library(furrr); library(future)

# Trees
library(randomForest); library(gbm); library(xgboost); library(caret)
library(rpart); library(rpart.plot)
```


### 1 Introduction

**1.1. Background**

This report examines the impact of AI-assisted coding tools on coding discussion forums. Large language models (LLM) have rapidly gained popularity and become integrated into daily life. In particular, programmers now frequently use AI-assisted coding tools such as ChatGPT, Gemini Code Assist, and GitHub Copilot in their workflows. The capabilities of these AI tools drew widespread attention when ChatGPT launched in November 2022, reaching over 100 million users within two months (Wikipedia contributors, 2024). To capture the short-term trend of this development, we restricted the analysis timeframe to 2021–2025, allowing for a comparison before and after ChatGPT’s release.

**1.2 Dataset**

The analysis focuses on Stack Overflow's public dataset, available through the Stack Exchange API. Stack Overflow is one of the largest question-and-answer platforms for programmers, with over 23 million users and nearly 60 million posts as of March 2024 (Wikipedia contributors, 2024). The API provides access to contents and interactions on Stack Overflow through entities such as Answers, Badges, and Comments (Stack Exchange, n.d.). For this study, we specifically examine the Posts, Tags, and Users data. Posts include both questions and answers, while Tags are the topics to categorize and organize content (Stack Overflow Teams, 2024). These data offer valuable insights into user engagement, content quality, and the evolving landscape of Stack Overflow over time.

**1.3 Objective**

To investigate the impact of AI-assisted coding tools on coding discussion forums, we focus on two key questions:

1. Have AI tools reduced reliance on Stack Overflow for coding assistance?
2. How have AI tools influenced the structure and content of forum posts?

By analyzing these aspects, we aim to infer the effects of ChatGPT’s launch on Stack Overflow’s users and content, providing a preliminary answer to the research question. (See <a href="https://github.com/Sophie-X31/JSC370-Final-Project" target="_blank">project repository</a> for data, code, and additional materials.)

### 2 Methods

**2.1 Acquire Data**

The Stack Overflow’s public data was retrieved using the Stack Exchange API v2.3. Since anonymous API users have a daily quota limit, we registered for an API key (StackApps, n.d.). As instructed in the documentation, custom filters were created through the API to extract datasets with specific attributes. Once the desired filter was generated as an encoded string, we pass it along with the other parameters, `order`, `sort`, `site`, `fromdate`, `todate`, `page`, and `pagesize` to the API request. To ensure an even distribution of posts from 2021 to 2025, we generated a sequence of start and end dates for each month between 2021-01-01 and 2025-01-01, making separate requests for each month. We fixed pagesize at 100 and merged the retrieved monthly data to create a balanced dataset over the timeframe. For user data, we used a timeframe from 2021-01-01 to 2024-04-01, constrained by API quota limits. For tags, since the Stack Exchange database contains approximately 65,000 tags, we retrieved the entire dataset without filtering. In total, we collected three datasets—posts, users, and tags—amounting to approximately 97 MB. The data was stored as CSV files for easy reuse. The retrieval process was implemented using the `httr` package in R.

**2.2 Data Cleaning and Wrangling**

To ensure the dataset was in a usable format, we first inspected and summarized each dataset. We found that 93.77% of `location` data in the users dataset and 0.94% of `owner_id` data in the posts dataset were missing, while all other variables had complete data. Given the focus of this analysis, we omitted the `location` variable but retained `owner_id`. Next, we reviewed all variables, removing irrelevant ones and renaming the rest for clarity. Exploratory visualizations revealed highly correlated and duplicate variables, which we also removed. Additionally, we identified two article entries in the posts dataset that were neither questions nor answers and excluded them. Based on the distribution and usage of tags, we decided to retain only the 100 most popular tags for analysis.

We then consolidated variables into new variables tailored to our question. We introduced `lifespan`, calculated as the time gap between creation and last access/modification dates in both the users and posts datasets. Based on the distribution of lifespan, reputation, and total badge counts, we categorized users into three engagement levels: "Expert", "Experienced", and "Normal". Similarly, we classified posts into three engagement levels based on lifespan, vote counts, and score: "High", "Moderate", and "Low". We also introduced a `quality` variable for posts, categorizing them as: "Good", "Normal", "Bad", and "Controversial" based on the total score and the ratio between upvotes and downvotes. Lastly, we joined the users and posts datasets by matching the post owner's user ID, creating a fourth dataset that facilitates the analysis of relationships between users and their posts. The cleaning and wrangling procedures were carried out using R packages including `tidyverse`, `dplyr`, and `data.table`.

**2.3 Natural Language Processing (NLP)** 

The text content of posts was also classified into new categorical variables. We identified error-related keywords (e.g., segmentation fault), actionable language (e.g., fix), and contextual phrases (e.g., ASAP), combining these into a score to categorize a post’s `intention` as either debugging or discussion. Similarly, we developed a `complexity` score based on posts’ lengths, markdown structure, number of technical terms, and vocabulary diversity, classifying posts as either basic or complex. In addition, we tokenized the post body and assigned one of the 100 most popular tags by matching tokens to tags and selecting the highest-frequency match. To identify whether a post was related to AI, we also applied Latent Dirichlet Allocation (LDA) topic modeling, with methodological details provided in section 2.5.1. 

Given the limitations of manual classification, we further utilized several machine learning models—Decision Tree, Random Forest, Bagging, Boosting, and XGBoost—to enhance the classification process. To prepare the post content for these models, we generated word embeddings by first constructing a term co-occurrence matrix, which captured the contextual proximity of words within a fixed window. This matrix served as input for the Global Vectors for Word Representation (GloVe) algorithm, which produced 50-dimensional semantic vectors for each word. Post-level embeddings were then computed by averaging the vectors, creating numeric feature vectors suitable for classification. To accelerate this computationally intensive task, we implemented parallel processing using high-performance computing (HPC) packages. The processed datasets were saved as CSV and RDS files for efficient reuse. This NLP pipeline relied on `stringr`, `topicmodels`, `tokenizers`, and `text2vec`, along with `furrr`, `future`, and `parallel` for distributed processing.

**2.4 Data Visualization**

For exploratory data analysis and visually evident statistical modeling, we employed a range of visualizations: violin plots, box plots, and barcode plots to depict distributions; scatter plots and correlation heatmaps to demonstrate correlation; bar plots to present rankings; and circle timeline plots, area charts, and calendar heatmaps to visualize change over time. Several of these graphics were made interactive, allowing filtering, selecting, and animating data across time, enhancing interpretability and engagement. The visualizations were developed using R packages including `ggplot2`, `ggcorrplot`, `plotly`, and `gridExtra`. The choice and design of visuals were inspired by Financial Time’s Visual Vocabulary (Financial Times Visual Vocabulary, n.d.) and Andy Kriebel’s Tableau workbook (Kriebel, n.d.).

**2.5 Statistical Modeling**

2.5.1 Classification For NLP

As outlined in section 2.3, a range of classification models was used to analyze and categorize post content. A brief overview of each method is provided below:

- **Latent Dirichlet Allocation (LDA)** is an unsupervised topic modeling method that identifies abstract topics in a collection of documents by modeling each document as a mixture of topics and each topic as a distribution over words. It infers these topic structures from a document-term matrix using probabilistic inference.
- **Decision Tree** is a supervised learning method that splits the data into branches based on feature values to make predictions. It works by recursively dividing the dataset using rules that maximize the separation of target classes, forming a tree structure where each leaf represents a final decision or prediction.
- **Random Forest** is an ensemble method that builds multiple decision trees on random subsets of the data and features, and combines their outputs to make more accurate and robust predictions. It reduces overfitting by averaging the predictions of diverse trees.
- **Bagging** is an ensemble technique that improves model stability and accuracy by training multiple models on different random bootstrap samples of the data and aggregating their predictions. The technique is used on the decision trees to mitigate the high variance.
- **Boosting** is another ensemble technique that builds models sequentially, where each new model focuses on correcting the errors made by the previous ones. It combines the outputs of many weak learners to form a strong overall model, often improving performance on difficult prediction tasks. The technique is again used on the decision trees.
- **XGBoost** is a highly efficient and scalable implementation of gradient boosting that builds decision trees sequentially to minimize a regularized loss function. It includes optimizations like tree pruning, parallelization, and regularization to enhance accuracy and prevent overfitting.

LDA was applied to extract latent topics from post bodies to determine whether a post was AI-related. For classifying post characteristics such as engagement, quality, complexity, and intention, we evaluated the performance of Decision Tree, Random Forest, Bagging, Boosting, and XGBoost models. Among these, the Boosting model achieved the best performance, with the lowest Root Mean Squared Error (0.3603) and the highest R-squared value (0.3000). This final model was configured with a Gaussian distribution, 1,000 boosting rounds, a learning rate of 0.5, and five-fold cross-validation. Implementation of these models was supported by the `rpart`, `randomForest`, `gbm`, `xgboost`, and `caret` packages in R.

2.5.2 Model Selection

To address the report’s core research questions, we transformed the creation timestamps of posts and users into daily intervals to facilitate time series analysis of posting and user activity trends. Alongside this continuous time predictor, we created a binary categorical variable, `turnpoint`, to indicate whether each observation occurred before or after the launch of ChatGPT (2022-11-01), enabling inferential comparisons. To model changes in post counts over time, we used models based on Negative Binomial distribution with a log link function to account for potential over-dispersion in count data. The following statistical models were considered:

- **Generalized Linear Models (GLMs)** extend traditional linear regression by allowing for non-normal error distributions and using link functions to relate the predictors to the response variable. **Generalized Linear Mixed Models (GLMMs)** build on GLMs by incorporating random effects to model grouped or nested data.
- **Generalized Additive Models (GAMs)** introduce flexibility by applying smooth functions to predictors, allowing the modeling of non-linear relationships. **Generalized Additive Mixed Models (GAMMs)** combine this flexibility with random effects.

To analyze the distribution of users across engagement levels, we treated user proportions as a compositional response and applied **Dirichlet Regression**. This model accommodates multivariate proportion data that sum to one and captures dependencies among components while modeling their relationships with covariates. For modeling the ratio of complex to basic posts, we considered both a Binomial GAM with a log-odds link function and a **Beta Regression** model. The latter offers flexibility in modeling proportional data constrained between 0 and 1. Across all GAM models, we used 30 basis functions (k = 30) to balance smoothness and model flexibility. Model fitting was performed using a suite of R packages, including `mgcv`, `glmmTMB`, `DirichletReg`, and `betareg`.

2.5.3 Test Statistics

Model evaluation was conducted using a range of statistical tests and performance metrics:

- The **Z-statistic**, used in GLMs and GLMMs, tests whether individual model coefficients differ significantly from zero, under the assumption of asymptotic normality.
- The **Chi-square test** is employed in GAMs to evaluate whether adding a smooth term significantly improves model fit by comparing deviance reductions.
- **Adjusted R-squared** quantifies the proportion of variance explained by a model, adjusted for the number of predictors. It penalizes model complexity to prevent overfitting and is calculated as $1-(\sum_{i=1}^n (\hat{y}_i-y_i)^2) / (\sum_{i=1}^n (y_i-\bar{y}_i)^2)$.
- For models where traditional R-squared is not defined (e.g., GLMs, GLMMs, Dirichlet and Beta regressions), we used **McFadden’s pseudo R-squared**, which compares the log-likelihoods of the fitted model and a null model to assess explanatory power.
- For classification model comparisons, we used **Root Mean Squared Error (RMSE)** to quantify prediction accuracy. RMSE is calculated as $\sqrt{\frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)^2}$.

While most statistics are provided automatically via R’s `base::summary()` function, the pseudo R-squared was obtained using the `pscl` package.

[comment]: <> (Step 1: Retrieve and Load Data)

```{r api_function, eval=TRUE, echo=FALSE, message=FALSE}
api_key <- "rl_sV67BVvG8ogoeRPbhaTc8oQZm"
base_url <- "https://api.stackexchange.com/2.3"

fetch_monthly_data <- function(endpoint, params, start_date = NULL, end_date = NULL, pagesize = NULL, pages = NULL) {
  all_data <- list()
  params$key <- api_key
  url <- paste0("https://api.stackexchange.com/2.3", endpoint)

  if (!is.null(start_date) & !is.null(end_date)) {
    fromdate <- as.numeric(as.POSIXct(start_date, format = "%Y-%m-%d"))
    todate <- as.numeric(as.POSIXct(end_date, format = "%Y-%m-%d"))
    params$fromdate <- fromdate
    params$todate <- todate
  }
  
  if (!is.null(pagesize) & !is.null(pages)) {
    params$pagesize <- pagesize
    for (page in 1:pages) {
      params$page <- page
      response <- httr::GET(url, query = params)
      
      status_code <- httr::status_code(response)
      content <- httr::content(response)
      if (status_code != 200) {
          print(status_code)
          print(content)
      }
      
      data <- content$items
      all_data <- append(all_data, data)
      Sys.sleep(2)
    }
  }
  else {
    response <- httr::GET(url, query = params)
    all_data <- append(all_data, data)
  }
  return(all_data)
}
```

```{r api_helper, eval=TRUE, echo=FALSE, message=FALSE}
generate_months <- function(start_date, end_date) {
  seq.Date(as.Date(start_date), as.Date(end_date), by = "month")
}
months <- generate_months("2021-01-01", "2025-01-01")
```

```{r create_filter, eval=TRUE, echo=FALSE, message=FALSE}
custom_filter <- function(include, base, unsafe) {
  url <- paste0(base_url, "/filters/create")
  params <- list(include=include, base=base, unsafe=unsafe, key = api_key)
  response <- httr::GET(url, query = params)
  filter_detail <- httr::content(response)$items
  return(filter_detail[[1]]$filter)
}
```

```{r fetch_posts, eval=TRUE, echo=FALSE, message=FALSE}
if (!file.exists("../data/posts.csv")) {
  include <- "post.up_vote_count;post.down_vote_count;post.body_markdown"
  posts_filter <- custom_filter(include, "default", FALSE)
  data <- list()
  for (i in 1:(length(months) - 1)) {
    from_date <- months[i]
    to_date <- months[i + 1]
    monthly_data <- fetch_monthly_data(
      endpoint = "/posts",
      params = list(order = "desc", 
                    sort = "creation", 
                    site = "stackoverflow", 
                    filter = posts_filter),
      start_date = from_date,
      end_date = to_date,
      pagesize = 100,
      pages = 10
    )
    print(from_date)
    data <- append(data, monthly_data)
    print(length(data))
  }
  df <- bind_rows(lapply(data, as.data.frame))
  write.csv(df, "../data/posts.csv", row.names = FALSE)
} else {
  posts_df <- fread("../data/posts.csv")
}
```

```{r fetch_users, eval=TRUE, echo=FALSE, message=FALSE}
if (!file.exists("../data/users.csv")) {
  include <- "user.answer_count;user.down_vote_count;user.question_count;user.up_vote_count;user.viewcount"
  users_filter <- custom_filter(include, "default", FALSE)
  data <- list()
  for (i in 1:(length(months) - 1)) {
    from_date <- months[i]
    to_date <- months[i + 1]
    user_data <- fetch_monthly_data(
      endpoint = "/users",
      params = list(order = "desc", 
                    sort = "creation", 
                    site = "stackoverflow",
                    filter = users_filter),
      start_date = from_date,
      end_date = to_date,
      pagesize = 100,
      pages = 5
    )
    print(from_date)
    data <- append(data, user_data)
    print(length(data))
  }
  df <- bind_rows(lapply(data, as.data.frame))
  df <- df |> select(-matches("^(posted_by_collectives|collectives)"))
  write.csv(df, "../data/users.csv", row.names = FALSE)
} else {
  users_df <- fread("../data/users.csv")
}
```

```{r fetch_tags, eval=TRUE, echo=FALSE, message=FALSE}
if (!file.exists("../data/tags.csv")) {
  data <- fetch_monthly_data(
    endpoint = "/tags",
    params = list(order = "desc", 
                  sort = "popular", 
                  site = "stackoverflow"),
    pagesize = 100,
    pages = 660
  )
  df <- bind_rows(lapply(data, as.data.frame))
  write.csv(df, "../data/tags.csv", row.names = FALSE)
} else {
  tags_df <- fread("../data/tags.csv")
}
```

[comment]: <> (Step 2: Wrangle Data)

```{r wrangle_posts, eval=TRUE, echo=FALSE, message=FALSE}
# Rename and Select Variables
colnames(posts_df) <- c("rm_account_id", "rm_reputation", "owner_id",
                        "rm_user_type", "rm_pfp", "rm_username",
                        "rm_user_link", "downvote_count", "upvote_count",
                        "score", "last_activity_date", "creation_date",
                        "post_type", "post_id", "body",
                        "rm_link", "rm_user_ar", "rm_last_edit")
posts_df <- posts_df |> select(-matches("^rm"))
```

```{r wrangle_users, eval=TRUE, echo=FALSE, message=FALSE}
# Rename and Select Variables
colnames(users_df) <- c("bronze_count", "silver_count", "gold_count",
                        "downvote_count", "upvote_count", "answer_count",
                        "question_count", "rm_acc_id", "rm_employee",
                        "last_access_date", "rep_year", "rep_quarter",
                        "rep_month", "rep_week", "rep_day",
                        "reputation", "creation_date", "rm_type",
                        "user_id", "rm_link", "rm_pfp",
                        "rm_name", "rm_last_modified", "location",
                        "rm_url")
users_df <- users_df |> select(-matches("^rm"))
```

```{r wrangle_tags, eval=TRUE, echo=FALSE, message=FALSE}
# Rename and Select Variables
tags_df <- tags_df |>
  select(-matches("^Id|Id$")) |>
  rename("name" = "TagName", "count" = "Count")
```

[comment]: <> (Step 2.5: Inspect Data)

```{r variables, eval=FALSE, echo=FALSE, message=FALSE}
# See All Variables
print("Users: ")
colnames(users_df)
print("Posts: ")
colnames(posts_df)
print("Tags: ")
colnames(tags_df)
```

```{r missing_data, eval=FALSE, echo=FALSE, message=FALSE}
# Check Missing Values in Each Dataset
user_missing <- colSums(is.na(users_df))
user_missing <- user_missing[user_missing > 0]
round((user_missing / nrow(users_df)) * 100, 2)

post_missing <- colSums(is.na(posts_df))
post_missing <- post_missing[post_missing > 0]
round((post_missing / nrow(posts_df)) * 100, 2)

colSums(is.na(tags_df))
```

```{r inspect_data, eval=FALSE, echo=FALSE, message=FALSE}
# User: Investigate the Distribution of Reputation
print(summary(users_df[, c("reputation", "rep_year", "rep_quarter", 
                           "rep_month", "rep_week", "rep_day")]))

thresholds <- c(10, 25, 50, 75, 100, 250, 500, 1000, 2000)
counts <- sapply(thresholds, function(x) sum(users_df$reputation > x))
threshold_summary <- data.frame(Threshold = thresholds, Count = counts)
print(threshold_summary)
```

```{r inspect_data2, eval=FALSE, echo=FALSE, message=FALSE}
# User: Check Downvotes vs Upvotes
summary(users_df[, c("bronze_count", "silver_count", "gold_count",
                     "upvote_count", "downvote_count")])

downvoters <- users_df |> filter(downvote_count > 0)
print(round((nrow(downvoters) / nrow(posts_df)) * 100, 2))
```

```{r inspect_data3, eval=FALSE, echo=FALSE, message=FALSE}
# Posts: Check Post Type
unique(posts_df$post_type)
posts_df |>
  filter(post_type == "article") |>
  head()
```

```{r inspect_data4, eval=FALSE, echo=FALSE, message=FALSE}
# Posts: Investigate Distribution of Lifespan
posts_df$lifespan <- as.numeric(difftime(posts_df$last_activity_date, 
                                         posts_df$creation_date, units = "days"))
summary(posts_df$lifespan)
print(round((nrow(posts_df |> filter(lifespan == 0)) / nrow(posts_df)) * 100, 2))

thresholds <- c(7, 14, 30, 120, 180, 365, 365*2, 365*3)
counts <- sapply(thresholds, function(x) sum(posts_df$lifespan > x))
threshold_summary <- data.frame(Threshold = thresholds, Count = counts)
print(threshold_summary)
```

```{r inspect_data5, eval=FALSE, echo=FALSE, message=FALSE}
# Posts: Score around Zero
controversial <- posts_df |> 
  filter (1/10 < upvote_count / downvote_count & upvote_count / downvote_count < 10)
print(round(nrow(controversial)/nrow(posts_df) * 100, 2))
```


### 3 Results

**3.1 Exploratory Data Analysis**

3.1.1 Post

Most of the variables measuring engagement and quality in the users and posts dataset exhibit strong skewness. Notably, 45.16% of posts are active for less than 24 hours, while 9.07% are active for more than a week.  As illustrated in Figure 1, both distributions have a light, long right tail, emphasizing this skewness.

[comment]: <> (Step 3: Exploratory Data Analysis)

```{r color_palette, eval=TRUE, echo=FALSE, message=FALSE}
color_pal <- c("#CACAAA", "#EEC584", "#C8AB83", "#55868C", "#7F636E")
color_shade <- c("#A1A168", "#E29A28", "#9C7744", "#9ABCC1", "#B7A4AC")
eight_pal <- c("#BC2C1A", "#8C2F39", "#7F636E", "#CACAAA", "#EEC584", "#C8AB83", "#55868C", "#22333B")
```

```{r eda2, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Create Lifespan
users_df$lifespan <- as.numeric(difftime(users_df$last_access_date, 
                                         users_df$creation_date, units = "days"))
users_life <- users_df |> select(lifespan) |> mutate(type = "User")
posts_df$lifespan <- as.numeric(difftime(posts_df$last_activity_date, 
                                         posts_df$creation_date, units = "days"))
posts_life <- posts_df |> select(lifespan) |> filter(lifespan > 7) |> mutate(type = "Post")
lifespan_df <- bind_rows(users_life, posts_life)

# Violin Plot
ggplot(lifespan_df, aes(x = type, y = lifespan, fill = type)) +
  geom_violin(trim = FALSE, alpha = 0.7) +
  coord_flip() +
  scale_fill_manual(values = c("User" = color_shade[5], "Post" = color_pal[5])) +
  labs(title = "Distribution of Lifespan: Users vs Posts",
       x = "Category", y = "Lifespan (days)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        plot.caption = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "none")
```
<center>
Figure. 1 Violin plots illustrating the lifespan distribution of posts that have been active for over a week (displayed at the bottom) along with the active lifespan of users (shown at the top). 
</center>

<br>
Only 5.65% of posts are controversial, meaning they receive a significant number of both upvotes and downvotes. While one might expect a negative correlation between upvotes and downvotes, Figure 2 suggests otherwise. The line of best fit indicates that posts with more upvotes tend to also receive more downvotes, possibly due to the polarizing nature of controversial posts.

```{r eda4, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}
ggplot(posts_df, aes(x = upvote_count, y = downvote_count, size = score)) +
  geom_point(alpha = 0.5, color = color_pal[4]) +
  geom_smooth(method = "lm", color = color_shade[4], se = FALSE, alpha=0.6) +
  scale_size_continuous(range = c(1, 10), name = "Score") +
  labs(title = "Trend in Post Upvotes vs. Downvotes", 
       x = "Upvote Count", y = "Downvote Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        plot.caption = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "none")
```
<center>
Figure. 2 Scatter plot comparing the number of upvotes and downvotes received by each post, with the size of the points indicating the score (computed as upvotes minus downvotes). 
</center>

<br>
The skewness could be explained by the possibility that a small proportion of basic, broadly useful questions generate high engagement, while the majority of questions are niche-specific and engage only a small subset of users. Additionally, controversial posts may attract high engagement due to their inherently divisive nature.

3.1.2 User

A similar skewed distribution is observed in the users dataset. Users are far more likely to upvote posts as a form of agreement or encouragement, while only 0.04% have ever downvoted a post. Badge counts also reflect this imbalance, up to the third quantile, users hold zero silver or gold badges, as well as zero upvotes or downvotes. User reputation follows an extreme skew as shown in Figure 3, with the minimum, median, and third quantile all valued at 1, while the maximum reaches 4,341.

```{r eda1, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Filter Users with Non-Minimal Reputation
rep_user <- users_df |> 
  select(user_id, reputation, rep_year, rep_quarter, rep_month, rep_week) |>
  filter(reputation > 10)
rep_change <- rep_user |>
  filter(!(rep_year == 0 & rep_quarter == 0 & rep_month == 0 & rep_week == 0)) |>
  mutate(across(c(rep_year, rep_quarter, rep_month, rep_week), 
                ~ ifelse(. == 0, . + 1e-3, .)))

# Reshape Data
rep_change_long <- rep_change |> 
  pivot_longer(cols = c(rep_year, rep_quarter, rep_month, rep_week), 
               names_to = "Metric", 
               values_to = "Value") |>
  filter(reputation != 1335 & reputation != 4341)
rep_change_long$Metric <- factor(rep_change_long$Metric, 
  levels = c("rep_year", "rep_quarter", "rep_month", "rep_week"),
  labels = c("Year", "Quarter", "Month", "Week"))
```

```{r eda1c, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Barcode + Boxplot
ggplot(rep_change_long, aes(x = Metric, y = Value, color = Metric)) +
  geom_jitter(aes(color = Metric), alpha = 0.5) +
  geom_boxplot(aes(fill = Metric), outlier.shape = NA, alpha = 0.8) +
  geom_rug(data = rep_change_long, aes(x = "Reputation", y = reputation / 25), 
           sides = "l", color = "black", alpha = 0.6) +
  coord_flip() +
  scale_color_manual(name = "Metric", values = color_pal) +
  scale_fill_manual(name = "Metric", values = color_shade) +
  labs(title = "Change in Reputation Over Time", 
       x = "Time Granularity", 
       y = "Reputation Change") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        plot.caption = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "top",
        legend.direction = "horizontal",
        legend.box = "horizontal",
        legend.justification = "center",
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 8))
```

<center>
Figure. 3 Circle timelines showing the distribution of changes in reputation across various time intervals, with the total reputation represented as a barcode plot. Outliers (total reputation of 4341 and reputation change of 212) have been omitted from this representation. 
</center>

<br>
Figure 3 shows that the density distribution of change in reputation over months nearly coincides with weekly changes, and similarly, quarterly changes align with yearly trends. This is supported by Figure 4, where the correlation matrix shows that reputation changes over different time intervals are highly correlated (>0.95). Additionally, reputation is strongly correlated with answer count (0.85) and silver badge count (0.68).

```{r eda3, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Correlation Plot
numeric_users <- select(users_df, where(is.numeric)) |> select(-user_id)
corr_matrix <- cor(numeric_users, use = "complete.obs")
ggcorrplot(corr_matrix, method = "square", type = "lower",
           lab = TRUE, lab_size = 2.4,
           colors = c("white", "white", color_pal[4]),
           title = "Correlation Heatmap: User Features") + 
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        plot.caption = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 8))
```
<center>
Figure. 4 Correlation heatmap depicting the correlation between different variables within the user data. 
</center>

<br>
These findings suggest that a small percentage of users actively engage in discussions, while the majority passively browse and search for information.

3.1.3 Tag

The tags dataset is straightforward—the top 20 tags correspond to widely used programming languages and concepts. JavaScript ranks first, followed by Python and Java.

```{r eda5, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
tags_df |>
  arrange(across(count, desc)) |>
  head(20) |>
  ggplot(aes(fct_reorder(name, count), count)) +
  geom_bar(stat="identity", fill=color_pal[1]) +
  coord_flip() +
  labs(title = "Top 20 Most Popular Tags", x = "Tag", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        plot.caption = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "none")
```
<center>
Figure. 5 Bar plot ranking the popularity of tags. 
</center>

[comment]: <> (Step 3.5: Wrangle Data II)

```{r wrangle_data, eval=TRUE, echo=FALSE, message=FALSE}
# Wrangle Data Again Based on EDA Results
users_df <- users_df |> select(-matches("location|rep_day|rep_week|rep_quarter"))
posts_df <- posts_df |>
  filter(post_type != "article") |>
  mutate(post_type = factor(post_type, levels = c("answer", "question")))

users_df$badge_count <- users_df$bronze_count + users_df$silver_count + users_df$gold_count
posts_df$vote_count <- posts_df$upvote_count + posts_df$downvote_count

popular_tags <- tags_df |>
  arrange(across(count, desc)) |>
  head(100)
```

[comment]: <> (Step 4: Define Parameter of Interest)
[comment]: <> (Consolidate Variables)

```{r inspect_data6, eval=FALSE, echo=FALSE, message=FALSE}
# Define Classification Metric Based on Percentile
print_percentiles <- function(data, variable) {
  percentiles <- round(quantile(data[[variable]], probs = seq(0, 1, 0.05), na.rm = TRUE), 2)
  print(variable)
  print(percentiles)
}
```

```{r classify_user, eval=TRUE, echo=FALSE, message=FALSE}
# Classify Users
users_df <- users_df |> 
  mutate(engagement = case_when(
    (badge_count == 1 | reputation < 11) & (lifespan > 60 & lifespan < 365 * (2 + 1/4)) ~ "experienced",
    (badge_count >= 2 | reputation >= 11) | lifespan > 365 * (2 + 1/4) ~ "expert",
    TRUE ~ "normal"))
users_df <- users_df |> 
  mutate(across(c(engagement), as.factor))
```

```{r classify_post, eval=TRUE, echo=FALSE, message=FALSE}
# Classify Posts
posts_df <- posts_df |> 
  mutate(engagement = case_when(
    lifespan > 5 | vote_count >= 2 ~ "high",
    lifespan < 1 | vote_count == 0 ~ "low",
    TRUE ~ "moderate"))

posts_df <- posts_df |> 
  mutate(quality = case_when(
    1/10 < upvote_count / downvote_count & upvote_count / downvote_count < 10 ~ "controversial",
    score >= 2  ~ "good",
    score < 0 ~ "bad",
    TRUE ~ "normal"))

posts_df <- posts_df |> 
  mutate(across(c(engagement, quality), as.factor))
```

[comment]: <> (Classify Post Content)

```{r classify_tag, eval=TRUE, echo=FALSE, message=FALSE}
key_words <- c("AI", "Artificial Intelligence", "ML", "Machine Learning", 
               "GPT", "Llama", "Claude", "DeepSeek", "DALL-E", "Gemini",
               "LLM", "Large Language Model", "Neural Network", "SVM",
               "Deep Learning", "DL", "Reinforcement Learning", "RL",
               "Natural Language Processing", "NLP", "fine-tun", "fine tun",
               "train model", "backprop", "back propagation", "gradient descent")

if (!file.exists("../data/processed_posts.csv")) {
  classify_tag <- function(posts_df, tags) {
    post_tags <- vector("character", nrow(posts_df))
    post_relevance <- logical(nrow(posts_df))
    
    for (i in 1:nrow(posts_df)) {
      # Tokenization
      tokens <- unlist(str_split(tolower(posts_df$body[i]), "\\W+"))
      tokens <- tokens[!tokens %in% stopwords("en")]
      tokens <- tokens[tokens != ""]
      tokens <- tokens[tokens != "quot"]
      tokens <- tokens[!grepl("^\\d+$", tokens)]
      token_df <- data.frame(word = tokens, stringsAsFactors = FALSE)
      token_freq <- token_df |>
        group_by(word) |>
        summarise(frequency = n()) |>
        arrange(across(frequency, desc)) 
      
      # Match Tags
      tag_matches <- popular_tags |>
        filter(tolower(name) %in% tolower(token_freq$word)) |>
        select(-"count") |>
        mutate(count = {
          counts <- list(100)
          for (i in seq_along(name)) {
            counts[i] <- sum(token_freq$frequency[token_freq$word == tolower(name[i])])
          }
          counts
        })
      
      # Classify Tag
      if (nrow(tag_matches) == 0) {
        post_tags[i] <- "None"
      }
      else {
        best_match <- tag_matches |>
          arrange(desc(count)) |>
          slice(1) |>
          pull(name)
        post_tags[i] <- best_match
      }
      
      # LDA Topic Modeling
      dtm <- DocumentTermMatrix(token_freq) |> as.matrix()
      lda_model <- LDA(dtm, k = 3, control=list(seed=42))
      lda_topics <- topics(lda_model)
      
      # Classify Relevance
      top_terms <- terms(lda_model, 5)
      top_words <- unique(unlist(top_terms))
      topic_match <- any(sapply(key_words, function(keyword) keyword %in% top_words))
      keyword_match <- any(token_freq$word %in% key_words)
      post_relevance[i] <- (keyword_match | topic_match)
  
      # Progress Bar
      if (i %% 100 == 0) {
        cat("Processed", i, "posts...\n")
      }
    }
    
    return(data.frame(tag = post_tags, relevance = post_relevance))
  }
  result <- classify_tag(posts_df, popular_tags)
  posts_df$tag <- result$tag
  posts_df$relevance <- result$relevance
} else {
  posts_df <- fread("../data/processed_posts.csv")
  posts_df <- posts_df |>
    mutate(
        across(c(post_type, engagement, quality, 
                 intention, complexity, tag, relevance), as.factor),
        across(c(owner_id, downvote_count, upvote_count, 
                 score, post_id, vote_count), as.integer),
        across(c(lifespan, complexity_score, debug_score), as.numeric),
        last_activity_date = as.POSIXct(last_activity_date, format = "%Y-%m-%d %H:%M:%S"),
        creation_date = as.POSIXct(creation_date, format = "%Y-%m-%d %H:%M:%S"))
}
```

```{r classify_type, eval=TRUE, echo=FALSE, message=FALSE}
classify_type <- function(body) {
  # Specific Error Message
  error_keywords <- c("error", "exception", "bug", "stack trace", 
                      "failed", "crash", "segmentation fault",
                      "null pointer", "memory leak")
  error_count <- sum(str_detect(body, fixed(error_keywords)))
  
  # Actionable Language
  actionable_phrases <- c("fix", "resolve", "help")
  actionable_count <- sum(str_detect(body, fixed(actionable_phrases)))
  
  # Contextual Urgency
  urgency_phrases <- c("ASAP", "urgent", "immediate")
  urgency_count <- sum(str_detect(body, fixed(urgency_phrases)))
  
  # Compute Score
  debug_score <- error_count * 5 + actionable_count * 3.5 + urgency_count * 1.5
  return(debug_score)
}

if (!file.exists("../data/processed_posts.csv")) {
  posts_df$debug_score <- sapply(posts_df$body, classify_type)
  posts_df <- posts_df |>
    mutate("intention" = factor(case_when(
      debug_score > 3.5 ~ "Debug",
      debug_score <= 3.5 ~ "Discussion"
    )))
}
```

```{r classify_complexity, eval=TRUE, echo=FALSE, message=FALSE}
classify_complexity <- function(posts_df) {
  complexity_score <- list(nrow(posts_df))
  for (i in 1:nrow(posts_df)) {
    body <- posts_df$body[i]
    
    # Length of the post
    tokens <- str_split(tolower(body), "\\W+")[[1]]
    word_count <- length(tokens)
    
    # Average sentence length
    sentence_count <- str_count(body, "[.!?]")
    avg_sentence_length <- ifelse(sentence_count == 0, 0, word_count / sentence_count)
    
    # Markdown Structure
    heading_count <- str_count(body, "##|###")
    bullet_point_count <- str_count(body, "-|\\*")
    
    # Vocabulary diversity
    unique_words <- length(unique(tokens))
    vocab_diversity <- unique_words / word_count
    
    # Readability score
    #readability_score <- textstat_readability(body)$Flesch
    
    # Tag Terms
    tech_term_count <- sum(str_detect(body, fixed(popular_tags$name, ignore_case = TRUE)))
    
    # Compute Score
    complexity_score[i] <- word_count * 2.5 + avg_sentence_length * 1 +
      vocab_diversity * 2 + heading_count * 1.5 + bullet_point_count * 1.5 + 
      tech_term_count * 1.5 # + readability_score * 2
    
    # Progress Bar
    if (i %% 100 == 0) {
      cat("Processed", i, "posts...\n")
    }
  }
  return(complexity_score)
}

if (!file.exists("../data/processed_posts.csv")) {
  result <- classify_complexity(posts_df)
  posts_df$complexity_score <- unlist(result)
  posts_df <- posts_df |>
    mutate("complexity" = factor(case_when(
      complexity_score > 800 ~ "Complex",
      complexity_score <= 800 ~ "Basic"
    )))
  write.csv(posts_df, "processed_posts.csv", row.names = FALSE)
}
```

```{r merge_data, eval=TRUE, echo=FALSE, message=FALSE}
# Merge Dataset
posts_wrangled <- posts_df |>
  select(post_id, owner_id, post_type, creation_date, engagement, quality, body)
users_wrangled <- users_df |>
  select(user_id, creation_date, engagement)

merged_df <- inner_join(posts_wrangled, users_wrangled, by = c("owner_id" = "user_id"))
merged_df <- merged_df |> 
  rename_with(~ gsub("(.*)\\.x$", "post_\\1", .), .cols = everything()) |> 
  rename_with(~ gsub("(.*)\\.y$", "owner_\\1", .), .cols = everything())
merged_df <- merged_df |>
  rename("post_quality" = "quality", "post_body" = "body")
```

[comment]: <> (Illustrate Post Content Classification)

```{r create_embedding, eval=TRUE, echo=FALSE, message=FALSE}
if (!file.exists("../data/post_embeddings.rds")) {
  # Tokenization
  tokens_df <- posts_df |>
    select(post_id, body) |>
    mutate(body = gsub("<.*?>", "", body)) |>
    unnest_tokens(word, body) |>
    filter(!str_detect(word, "[[:digit:]]+")) |>
    filter(!str_detect(word, "quot")) |>
    filter(!word %in% stopwords("en"))
  
  # Mimic Vocab (Problem with create_vocabulary())
  term_count_df <- tokens_df |> count(word) |> arrange(desc(n))
  doc_count_df <- tokens_df |> distinct(post_id, word) |> count(word)
  vocab <- left_join(term_count_df, doc_count_df, by = "word") |>
    rename(term = word, term_count = n.x, doc_count = n.y)
  attr(vocab, "ngram") <- c(1, 1)
  names(attr(vocab, "ngram")) <- c("ngram_min", "ngram_max")
  attr(vocab, "document_count") <- nrow(posts_df)
  attr(vocab, "stopwords") <- character(0)
  attr(vocab, "sep_ngram") <- "_"
  
  # Embedding Matrix
  tokens_list <- tokens_df |> group_by(post_id) |> summarise(tokens = list(word))
  vectorizer <- vocab_vectorizer(vocab)
  tcm <- create_tcm(itoken(tokens_list$tokens), vectorizer, skip_grams_window = 5)
  glove <- GlobalVectors$new(rank = 50, x_max = 10)
  wv_main <- glove$fit_transform(tcm, n_iter = 10)
  word_vectors <- wv_main + t(glove$components)

  saveRDS(word_vectors, "../data/word_vectors.rds")
}
```

```{r merge_embedding, eval=TRUE, echo=FALSE, message=FALSE}
if (!file.exists("../data/post_embeddings.rds")) {
  plan(multisession, workers = parallel::detectCores() - 1) 
  get_post_embedding <- function(post_text) {
    words <- unlist(strsplit(post_text, " "))
    words <- words[words %in% rownames(word_vectors)]
    if (length(words) > 0) {
      embedding <- colMeans(word_vectors[words, , drop = FALSE], na.rm = TRUE)
      stopifnot(length(embedding) == ncol(word_vectors))
      return(embedding)
    } else {
      return(rep(NA, ncol(word_vectors)))
    }
  }
  post_embeddings <- future_map(posts_df$body, get_post_embedding, .progress = TRUE)
} else {
  post_embeddings <- readRDS("../data/post_embeddings.rds")
}
post_embeddings <- do.call(rbind, post_embeddings)
embedding_df <- as.data.frame(post_embeddings)
colnames(embedding_df) <- paste0("dim_", seq_len(ncol(embedding_df)))
posts_df <- bind_cols(posts_df, embedding_df)
```

```{r data_split, eval=TRUE, echo=FALSE, message=FALSE}
# Split Data
set.seed(42)
train_ind <- sample(seq_len(nrow(users_df)), size = floor(0.7 * nrow(users_df)))
users_df_pred <- users_df |>
  mutate_if(is.POSIXct, as.integer) |>
  mutate_if(is.factor, ~ as.integer(.))
user_train <- users_df_pred[train_ind, ]
user_test <- users_df_pred[-train_ind, ]

train_ind <- sample(seq_len(nrow(posts_df)), size = floor(0.7 * nrow(posts_df)))
posts_df_pred <- posts_df |>
  select(-c(body, debug_score, complexity_score)) |>
  mutate_if(is.POSIXct, as.integer) |>
  mutate_if(is.factor, ~ as.integer(.))
post_train <- posts_df_pred[train_ind, ]
post_test <- posts_df_pred[-train_ind, ]
```

```{r find_model, eval=FALSE, echo=FALSE, message=FALSE}
compare_models <- function(variable, train, test, type) {
  formula <- as.formula(paste(variable, "~ ."))
  response <- test[[variable]]
  
  # Decision Tree
  tree <- rpart(formula, data = train, method = "anova", 
                control = rpart.control(minsplit = 10, minbucket = 3, cp = 0.01, xval = 10))
  optimal_cp <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
  tree <- prune(tree, cp = optimal_cp)
  tree_pred <- predict(tree, test)
  tree_rmse <- sqrt(mean((response - tree_pred)^2, na.rm = TRUE))
  tree_rsq <- 1 - sum((response - tree_pred)^2, na.rm = TRUE) / 
                 sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
  
  # Bagging
  bagging <- randomForest(formula, data = train, mtry = ncol(train) - 1, na.action = na.omit)
  bag_pred <- predict(bagging, test)
  bag_rmse <- sqrt(mean((response - bag_pred)^2, na.rm = TRUE))
  bag_rsq <- 1 - sum((response - bag_pred)^2, na.rm = TRUE) / 
                 sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
  
  # Random Forest
  rf <- randomForest(formula, data = train, na.action = na.omit)
  rf_pred <- predict(rf, test)
  rf_rmse <- sqrt(mean((response - rf_pred)^2, na.rm = TRUE))
  rf_rsq <- 1 - sum((response - rf_pred)^2, na.rm = TRUE) / 
                 sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
  
  # Boosting
  opt_boost <- gbm(formula, 
                   data = train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 1,
                   shrinkage = 0.5, 
                   cv.folds = 5, 
                   n.cores = 4, 
                   verbose = FALSE)
  optimal_trees <- gbm.perf(opt_boost, method = "cv", plot.it = FALSE)
  boost_pred <- predict(opt_boost, test, n.trees = optimal_trees, type = "response")
  boost_rmse <- sqrt(mean((response - boost_pred)^2, na.rm = TRUE))
  boost_rsq <- 1 - sum((response - boost_pred)^2, na.rm = TRUE) / 
                 sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
  
  # XGBoost
  train_control <- trainControl(method = "cv", number = 10, search = "grid", allowParallel = TRUE)
  tune_grid <- expand.grid(max_depth = 4, 
                           nrounds = (1:10) * 50, 
                           eta = 0.05, 
                           gamma = 0, 
                           subsample = 1,
                           min_child_weight = 1,
                           colsample_bytree = 0.6)
  
  xgb <- caret::train(formula, 
                      data = train,
                      method = "xgbTree",
                      trControl = train_control,
                      tuneGrid = tune_grid,
                      na.action = na.omit,
                      verbosity = 0)
  
  xgb_pred <- predict(xgb, test)
  xgb_rmse <- sqrt(mean((response - xgb_pred)^2, na.rm = TRUE))
  xgb_rsq <- 1 - sum((response - xgb_pred)^2, na.rm = TRUE) / 
                 sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
  
  # Comparison
  mod_compare <- data.frame(
    Model = c("Decision Tree", "Bagging", "Random Forest", "Boosting", "XGBoost"),
    RMSE = c(tree_rmse, bag_rmse, rf_rmse, boost_rmse, xgb_rmse),
    R_Squared = c(tree_rsq, bag_rsq, rf_rsq, boost_rsq, xgb_rsq)
  )
  knitr::kable(mod_compare, digits = 4, caption = "Table. # Model Performance Comparison")
}
```

```{r view_classification, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# User: engagement
response <- user_test$engagement
bagging <- randomForest(engagement ~ ., data = user_train, mtry = ncol(user_train) - 1, 
                        na.action = na.omit)
bag_pred <- predict(bagging, user_test)
bag_rmse <- sqrt(mean((response - bag_pred)^2, na.rm = TRUE))
bag_rsq <- 1 - sum((response - bag_pred)^2, na.rm = TRUE) / 
               sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
```

```{r view_classification2, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Post: engagement
response <- post_test$engagement
posts_df_convert <- posts_df_pred |>
  mutate(engagement = factor(engagement, labels = c("low", "high", "moderate")))
post_train <- posts_df_convert[train_ind, ]
post_test <- posts_df_convert[-train_ind, ]

if (!file.exists("../models/opt_boost")) {
  opt_boost <- gbm(engagement ~ ., 
                   data = post_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 1,
                   shrinkage = 0.5, 
                   cv.folds = 5, 
                   n.cores = 4, 
                   verbose = FALSE)
  saveRDS(opt_boost, "../models/opt_boost")
} else {
  opt_boost <- readRDS("../models/opt_boost")
}
optimal_trees <- gbm.perf(opt_boost, method = "cv", plot.it = FALSE)
boost_pred <- predict(opt_boost, post_test, n.trees = optimal_trees, type = "response")
response <- as.numeric(response)
boost_rmse <- sqrt(mean((response - boost_pred)^2, na.rm = TRUE))
boost_rsq <- 1 - sum((response - boost_pred)^2, na.rm = TRUE) / 
               sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
```

```{r view_classification3, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Post: quality
response <- post_test$quality
posts_df_convert <- posts_df_pred |>
  mutate(quality = factor(quality, labels = c("normal", "bad", "good", "controversial")))
post_train <- posts_df_convert[train_ind, ]
post_test <- posts_df_convert[-train_ind, ]

if (!file.exists("../models/opt_boost2")) {
  opt_boost2 <- gbm(quality ~ ., 
                   data = post_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 1,
                   shrinkage = 0.5, 
                   cv.folds = 5, 
                   n.cores = 4, 
                   verbose = FALSE)
  saveRDS(opt_boost2, "../models/opt_boost2")
} else {
  opt_boost2 <- readRDS("../models/opt_boost2")
}
optimal_trees2 <- gbm.perf(opt_boost2, method = "cv", plot.it = FALSE)
boost_pred2 <- predict(opt_boost2, post_test, n.trees = optimal_trees2, type = "response")
response <- as.numeric(response)
boost_rmse2 <- sqrt(mean((response - boost_pred2)^2, na.rm = TRUE))
boost_rsq2 <- 1 - sum((response - boost_pred2)^2, na.rm = TRUE) / 
               sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
```

```{r view_classification4, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Post: intention
response <- post_test$intention
posts_df_convert <- posts_df_pred |>
  mutate(intention = factor(intention, labels = c("Debug", "Discussion")))
post_train <- posts_df_convert[train_ind, ]
post_test <- posts_df_convert[-train_ind, ]

if (!file.exists("../models/opt_boost3")) {
  opt_boost3 <- gbm(intention ~ ., 
                   data = post_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 1,
                   shrinkage = 0.5, 
                   cv.folds = 5, 
                   n.cores = 4, 
                   verbose = FALSE)
  saveRDS(opt_boost3, "../models/opt_boost3")
} else {
  opt_boost3 <- readRDS("../models/opt_boost3")
}
optimal_trees3 <- gbm.perf(opt_boost3, method = "cv", plot.it = FALSE)
boost_pred3 <- predict(opt_boost3, post_test, n.trees = optimal_trees3, type = "response")
response <- as.numeric(response)
boost_rmse3 <- sqrt(mean((response - boost_pred3)^2, na.rm = TRUE))
boost_rsq3 <- 1 - sum((response - boost_pred3)^2, na.rm = TRUE) / 
               sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
```

```{r view_classification5, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Post: complexity 
response <- post_test$complexity
posts_df_convert <- posts_df_pred |>
  mutate(complexity = factor(complexity, labels = c("Basic", "Complex")))
post_train <- posts_df_convert[train_ind, ]
post_test <- posts_df_convert[-train_ind, ]

if (!file.exists("../models/opt_boost4")) {
  opt_boost4 <- gbm(complexity ~ ., 
                   data = post_train, 
                   distribution = "gaussian", 
                   n.trees = 1000, 
                   interaction.depth = 1,
                   shrinkage = 0.5, 
                   cv.folds = 5, 
                   n.cores = 4, 
                   verbose = FALSE)
  saveRDS(opt_boost4, "../models/opt_boost4")
} else {
  opt_boost4 <- readRDS("../models/opt_boost4")
}
optimal_trees4 <- gbm.perf(opt_boost4, method = "cv", plot.it = FALSE)
boost_pred4 <- predict(opt_boost4, post_test, n.trees = optimal_trees4, type = "response")
response <- as.numeric(response)
boost_rmse4 <- sqrt(mean((response - boost_pred4)^2, na.rm = TRUE))
boost_rsq4 <- 1 - sum((response - boost_pred4)^2, na.rm = TRUE) / 
               sum((response - mean(response, na.rm = TRUE))^2, na.rm = TRUE)
```

```{r visual_classification, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# User Engagement / Bagging
varImpPlot(bagging, 
           main = "Bagging Model: Variable Importance",
           sub = cap,
           col = "steelblue",
           pch = 16,
           cex.main = 0.9,
           cex.sub = 0.8,
           cex.lab = 0.8)
```


3.1.4 Classification 

As outlined in the method section, relying solely on manually defined metrics to classify post content can be limiting. We ran classification with the best-performing boosting model to gain deeper insights. Figure 6 presents the top five most influential features for each classification. Notably, the total number of votes and the active lifespan of a post emerged as strong indicators for predicting engagement level, while the number of downvotes played a significant role in determining content quality. Additionally, word embeddings (`dim_#`) helped distinguish between debugging and discussion posts, and the inferred intention also contributed to identifying its complexity.

```{r visual_classification2, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Extract Top 5 Variables
var_imp1 <- summary(opt_boost, plotit = FALSE) |> slice_max(order_by = rel.inf, n = 5)
var_imp2 <- summary(opt_boost2, plotit = FALSE) |> slice_max(order_by = rel.inf, n = 5)
var_imp3 <- summary(opt_boost3, plotit = FALSE) |> slice_max(order_by = rel.inf, n = 5)
var_imp4 <- summary(opt_boost4, plotit = FALSE) |> slice_max(order_by = rel.inf, n = 5)

# Plots
p1 <- ggplot(var_imp1, aes(x = reorder(var, rel.inf), y = rel.inf)) +
  geom_col(fill = color_pal[1]) +
  coord_flip() +
  labs(title = "Post Engagement", x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10))

p2 <- ggplot(var_imp2, aes(x = reorder(var, rel.inf), y = rel.inf)) +
  geom_col(fill = color_pal[1]) +
  coord_flip() +
  labs(title = "Post Quality", x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10))

p3 <- ggplot(var_imp3, aes(x = reorder(var, rel.inf), y = rel.inf)) +
  geom_col(fill = color_shade[1]) +
  coord_flip() +
  labs(title = "Post Intention", x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10))

p4 <- ggplot(var_imp4, aes(x = reorder(var, rel.inf), y = rel.inf)) +
  geom_col(fill = color_shade[1]) +
  coord_flip() +
  labs(title = "Post Complexity", x = NULL, y = NULL) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10))

# Labeling
x_axis_title <- textGrob("Relative Influence", gp = gpar(fontsize = 10))
y_axis_title <- textGrob("Variables", rot = 90, gp = gpar(fontsize = 10))
main_title <- textGrob("Variable Importance Across Boosting Models", 
                       gp = gpar(fontsize = 11, fontface = "bold"))
grid.arrange(p1, p3, p2, p4, nrow = 2, top = main_title,
             left = y_axis_title, bottom = x_axis_title)
```
<center>
Figure. 6 Variable importance plots highlighting key variables used for classifying posts.
</center>

[comment]: <> (Modeling Visualizations)

**3.2 Statistical Modeling**

3.2.1 Impact on Reliance

Recall we are interested in whether ChatGPT’s launch reduced programmers' reliance on Stack Overflow. Figure 7 shows a clear decline in basic question posts since 2022. The GAM suggests that time and complexity significantly impacted post counts ($p < 2e-16$), explaining 91.4% of the deviance. Relying on the interaction between `turnpoint` and complexity improved the GLM, achieving a pseudo R-squared of 0.9611 ($p < 2e-16$). These results suggest a decrease in basic coding questions, possibly due to the use of AI tools instead of forums for basic coding help.

```{r q1-1, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}
npost_basic <- posts_df |>
  filter(complexity == "Basic", post_type == "question") |>
  mutate(time_interval = floor_date(creation_date, "day")) |>
  group_by(time_interval) |>
  summarise(post_count = n(), .groups = 'drop')

p <- ggplot(data = npost_basic, aes(x = time_interval, y = post_count)) +
    geom_point(color = color_shade[5], 
               aes(text = paste("Post Count:", post_count, "<br>Date:", time_interval))) +
    geom_smooth(method = "loess", color = color_pal[5], linewidth = 0.8, se = FALSE) +
    labs(title = "Distribution of Basic Question Post Counts vs Time",
         x = "Time", y = "Post Count") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
          plot.caption = element_text(hjust = 0.5, size = 10),
          axis.text = element_text(size = 9),
          axis.title = element_text(size = 10),
          legend.position = "none")
ggplotly(p, tooltip = "text") |>
  layout(xaxis = list(rangeslider = list(visible = TRUE)))
```
<center>
Figure. 7 Scatter plot demonstrating the trend of basic question posts over time. Please use the slider at the bottom to zoom in on specific periods. 
</center>

<br>
A clear trend emerges when examining the complexity of posts. Figure 8 shows a consistent rise in the ratio of complex to basic posts. The Beta regression model indicates that both the time variable ($p \approx 0.0005$) and the `turnpoint` ($p < 2e-16$) significantly influenced this ratio, with the model explaining 71.67% of the deviance. These findings suggest that users are increasingly turning to forums for more complex coding inquiries and responses, possibly leaving simpler tasks to LLM coding assistance.

```{r q2-3, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Segment by Month
complexity_ratio <- posts_df |>
  mutate(year = year(creation_date), 
         month = factor(month(creation_date), levels = 1:12, labels = month.abb)) |>
  group_by(year, month, complexity) |>
  summarise(count = n(), .groups = 'drop') |>
  pivot_wider(names_from = complexity, values_from = count, values_fill = list(count = 0)) |>
  mutate(ratio = `Complex` / `Basic`)

# Calendar Heatmap
ggplot(complexity_ratio, aes(x = year, y = month, fill = ratio)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "#E0CFB8", high = color_shade[3]) +
  labs(title = "Distribution of Post Complexity Ratio Over Time",
       x = "Year", y = "Month", fill = "C/B Ratio") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        axis.text.x = element_text(size = 9),
        axis.text.y = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "right",
        panel.spacing = unit(0.5, "lines"))
```
<center>
Figure. 8 Calendar heatmap presenting the trend in the ratio of complex to basic posts over time, with the ratio represented as C/B (complex/basic). 
</center>

<br>
We also hypothesized that AI tools might reduce contributions from normal users. As shown in Figure 9, post activity among normal and experienced users rose from 2022 through mid-2023, followed by a noticeable drop, while expert users exhibited a steady decline in posting since 2021. This is contrary to our hypothesis. The GLM found that after ChatGPT's launch ($p < 2e-16$), normal user status ($p \approx 0.018$), and their interaction term ($p \approx 0.0407$) were all significant predictors of post counts. The GAM model explains 69.8% of the deviance, outperforming the GLM (pseudo R^2 \approx 0.3596). Note these results are based on the merged dataset instead of full datasets, which may introduce sample bias due to its smaller size.

```{r q1-2, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
npost_merged <- merged_df |>
  mutate(time_interval = floor_date(post_creation_date, "day")) |>
  group_by(time_interval, owner_engagement) |>
  summarise(post_count = n(), .groups = 'drop') |>
  complete(time_interval, owner_engagement, fill = list(post_count = 0))

ggplot(npost_merged, aes(x = time_interval, y = owner_engagement, 
                         size = post_count, color = owner_engagement)) +
  geom_point(alpha = 0.6) +
  scale_color_manual(values = color_pal) +
  scale_size(range = c(1, 16)) +
  labs(title = "Distribution of Post Counts vs Time By User Engagement",
       x = "User Engagement Category", y = "Time", size = "Post Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        plot.caption = element_text(hjust = 0.5, size = 10),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "none")
```
<center>
Figure. 9 Circle timelines illustrating the trend in the number of posts over time, influenced by the engagement level of the post owners, with circle sizes indicating the number of posts. 
</center>

<br>
Additionally, we explored whether the composition of user types has shifted. Figure 10 shows the share of experienced and normal users has steadily increased, while the share of expert users has declined drastically. The Dirichlet regression model confirms the significance of `turnpoint` ($p \approx 5.85e-7; p \approx 5.8e-13$) and the time variable ($p \approx 0.0080; p \approx 0.0113$) in impacting experienced and normal user shares, respectively. Before the `turnpoint` also significantly impacted the share of expert users ($p \approx 0.0005$), underscoring the influence of AI tools in reshaping platform participation. The model achieved a strong McFadden’s pseudo R-squared of 0.3675.

```{r q1-3, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Reshape Data
user_ratio <- users_df |>
  mutate(time_interval = floor_date(creation_date, "day")) |>
  group_by(time_interval, engagement) |>
  summarise(count = n(), .groups = 'drop') |>
  pivot_wider(names_from = engagement, values_from = count, values_fill = list(count = 0)) |>
  mutate(experienced_share = experienced / (experienced + expert + normal),
         expert_share = expert / (experienced + expert + normal),
         normal_share = normal / (experienced + expert + normal)) |>
  complete(time_interval, fill = list(experienced = 0, normal = 0, expert = 0))

user_ratio_long <- user_ratio |>
  select(-c(experienced, expert, normal)) |>
  rename(experienced = experienced_share,
         expert = expert_share,
         normal = normal_share) |>
  pivot_longer(cols = c(experienced, expert, normal),
               names_to = "engagement", 
               values_to = "share")
```

```{r q1-3c, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Area Chat
ggplot(user_ratio_long, aes(x = time_interval, y = share, fill = engagement)) +
  geom_area(alpha = 0.8) +
  scale_fill_manual(values = c(color_shade[5], "#9C818D", color_pal[5])) +  
  labs(title = "User Engagement Distribution Over Time",
       x = "Time", y = "User Share",
       fill = "Engagement Level") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
        axis.text = element_text(size = 9),
        axis.title = element_text(size = 10),
        legend.position = "top",
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 8))
```
<center>
Figure. 10 Area chart showing the change in the proportion of users across different engagement levels over time. </center>

<br>
In summary, AI-assisted coding tools appear to have reduced reliance on coding forums among beginner programmers asking basic questions. However, the engagement level of advanced users has declined.

3.2.2 Impact on Content Structure

Beyond engagement and reliance, we are also interested in examining whether LLM coding assistance have influenced the nature of content posted on coding forums. Figure 11 compares debugging and discussion-oriented posts, highlighting that the number of normal-quality debugging posts has slightly increased since 2023 ($p < 2e-16$), while good-quality discussion posts have declined over the same period ($p < 1.84e-6$). Other content categories remained relatively unchanged. The GAM including the interaction between post quality and intention explained 97.6% of the deviance in post volume.

```{r q2-1, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
# Select Categories
npost_intention <- posts_df |>
  mutate(time_interval = floor_date(creation_date, "day")) |>
  group_by(time_interval, quality, intention) |>
  summarise(post_count = n(), .groups = 'drop') |>
  complete(time_interval, quality, intention, fill = list(post_count = 0))

# Circle Timeline
p1 <- npost_intention |>
  filter(intention == "Debug") |>
  ggplot(aes(x = time_interval, y = quality, 
                           size = post_count, color = quality)) +
    geom_point(alpha = 0.6) +
    scale_color_manual(values = c(color_pal[1:4])) +
    scale_size(range = c(1, 12)) +
    labs(title = "Debug Post Counts Over Time by Post Quality",
         x = "Time Interval", y = "Post Quality", 
         size = "Post Count", color = "Post Quality") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
          axis.text = element_text(size = 9),
          axis.title = element_text(size = 10),
          legend.position = "none")

p2 <- npost_intention |>
  filter(intention == "Discussion") |>
  ggplot(aes(x = time_interval, y = quality, 
                           size = post_count, color = quality)) +
    geom_point(alpha = 0.6) +
    scale_color_manual(values = c(color_shade[1:4])) +
    scale_size(range = c(1, 12)) +
    labs(title = "Discussion Post Counts Over Time by Post Quality",
         x = "Time Interval", y = "Post Quality", 
         size = "Post Count", color = "Post Quality") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5, size = 11, face = "bold"),
          axis.text = element_text(size = 9),
          axis.title = element_text(size = 10),
          legend.position = "none")
```

```{r q2-1c, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center', warning=FALSE}
# Interactive Plot
p1_plotly <- ggplotly(p1)
p2_plotly <- ggplotly(p2)

# Set visibility: Show p1 By Default
all_traces <- c(p1_plotly$x$data, p2_plotly$x$data)
for (i in seq_along(all_traces)) {
  all_traces[[i]]$visible <- i <= length(p1_plotly$x$data)
}

# Create Dropdown
plotly_combined <- plotly::plot_ly()
plotly_combined$x$data <- all_traces
plotly_combined <- layout(
  plotly_combined,
  title = "Post Counts Over Time by Post Quality",
  xaxis = p1_plotly$x$layout$xaxis,
  yaxis = p1_plotly$x$layout$yaxis,
  showlegend = FALSE,
  updatemenus = list(
    list(
      type = "dropdown",
      active = 0,
      buttons = list(
        list(
          label = "Debug",
          method = "restyle",
          args = list("visible", c(rep(TRUE, length(p1_plotly$x$data)), rep(FALSE, length(p2_plotly$x$data))))
        ),
        list(
          label = "Discussion",
          method = "restyle",
          args = list("visible", c(rep(FALSE, length(p1_plotly$x$data)), rep(TRUE, length(p2_plotly$x$data))))
        )
      )
    )
  )
)

plotly_combined
```

<center>
Figure. 11 Circle Timelines reflecting the variation in the number of posts categorized by quality over time. Please use the button to select distributions related to discussion or debug posts. 
</center>

<br>
To further assess how post wording may have evolved, we projected word embeddings of post bodies into two dimensions. While this visualization does not provide direct semantic interpretation, it reveals visible shifts in clustering patterns over time, suggesting structural changes in how users compose their posts.

```{r q4, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
# Reduce Dimensions
if (!file.exists("../data/umap.rds")) {
  umap_result <- umap(na.omit(embedding_df))
  saveRDS(umap_result, file = "../data/umap.rds")
} else {
  umap_result <- readRDS("../data/umap.rds")
}
posts_cleaned <- posts_df |> filter(!if_any(starts_with("dim_"), is.na))
posts_cleaned$UMAP1 <- umap_result$layout[,1]
posts_cleaned$UMAP2 <- umap_result$layout[,2]
posts_cleaned$year <- lubridate::year(posts_cleaned$creation_date)

# Plot Change Over Time
plot_ly(posts_cleaned,
        x = ~UMAP1,
        y = ~UMAP2,
        frame = ~year,
        #text = ~paste("Post ID:", post_id, "<br>Year:", year),
        hoverinfo = "skip", #text
        type = "scatter",
        mode = "markers",
        marker = list(size = 12, opacity = 0.8, color = color_pal[5])) |>
  layout(title = "Change in Posts' Word Embeddings Over Time",
         xaxis = list(title = "UMAP1"),
         yaxis = list(title = "UMAP2"),
         legend = list(title = list(text = "Year"))) |>
  animation_opts(frame = 10, redraw = TRUE) |>
  animation_slider(currentvalue = list(prefix = "Year: ", font = list(size = 12)))
```

<center>
Figure. 12 Scatter plot illustrating the shift in post wording and structure over time based on word embedding projections. Please use the button to display the animated change through time.
</center>

<br>
Figure 13 demonstrates the examination of the impact of AI tools on posts in the top 10% of popular tags. It exhibits a significant decline in posts related to basic concepts such as lists and functions, while posts tagged with more complex topics like APIs have surged. The GLMM revealed that post timing relative to ChatGPT’s launch significantly contributed to the difference in post counts ($p < 2e-16$), while the GAMM emphasized the importance of the random effects of tags ($p < 2e-16$), explaining 86.8% of the deviance. These results suggest that while AI tools are increasingly used for routine coding issues, users continue to seek forum support for more advanced or nuanced programming topics.

```{r q3-3, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
top_tags <- posts_df |> filter(tag != "None") |> group_by(tag) |> count()
threshold <- round(quantile(top_tags$n, probs = seq(0.9, 1, 0.1), na.rm = TRUE), 2)[1]
top_tags <- top_tags |> filter(n > threshold)

npost_tag <- posts_df |>
  filter(tag %in% top_tags$tag) |>
  mutate(time_interval = floor_date(creation_date, "day"),
         year = year(creation_date)) |>
  group_by(year, time_interval, tag) |>
  summarise(post_count = n(), .groups = 'drop')
```

```{r q3-3c, eval=TRUE, echo=FALSE, message=FALSE, fig.align='center'}
plot_ly(npost_tag, 
        x = ~tag, 
        y = ~post_count, 
        color = ~tag,
        colors = eight_pal,
        frame = ~year,
        text = ~paste("<br>Post Count:", post_count, "<br>Time:", time_interval), 
        hoverinfo = "text",
        type = "scatter", 
        mode = "markers",
        marker = list(size = 8, opacity = 0.7)) |>
  layout(title = "Post Count Trends Over Time by Tag",
         xaxis = list(title = "Tag", categoryorder = "array", 
                      categoryarray = unique(npost_tag$tag)), 
         yaxis = list(title = "Post Count"),
         legend = list(title = list(text = "Tags"))) |>
  animation_opts(frame = 100, redraw = TRUE) |>
  animation_slider(currentvalue = list(prefix = "Time: ", font = list(size = 10)))
```

<center>
Figure. 13 Scatter plot illustrating the change in post counts associated with the most popular tags over time. Please use the button to display the animated changes through time.
</center>

<br>
In conclusion, the emergence of AI-assisted coding tools appears to encourage programmers to post more complex questions and discussions, rather than basic inquiries.

[comment]: <> (Run Models)

```{r modeling, eval=TRUE, echo=FALSE, message=FALSE}
turn_date <- "2022-11-01"
```

```{r modeling1, eval=FALSE, echo=FALSE, message=FALSE}
# Select Predictors
npost_basic <- posts_df |>
  filter(post_type == "question") |>
  mutate(time_interval = floor_date(creation_date, "day"),
         turnpoint = ifelse(time_interval < as.Date(turn_date), "before", "after")) |>
  group_by(time_interval, turnpoint, complexity) |>
  summarise(post_count = n(), .groups = 'drop')
npost_basic$time <- as.numeric(npost_basic$time_interval - min(npost_basic$time_interval))

# GLM
glmm <- glmmTMB(post_count ~ turnpoint * complexity, data = npost_basic, family = nbinom2)
summary(glmm)
pR2(glmm)

# GAM
summary(gam(post_count ~ s(time, k=30) + complexity, 
            data = npost_basic, family = nb(link = 'log'), method = 'ML'))
```


```{r modeling2, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Select Predictors
npost_merged <- npost_merged |>
  mutate(turnpoint = ifelse(time_interval < as.Date(turn_date), "before", "after"))
npost_merged$time <- as.numeric(npost_merged$time_interval - min(npost_merged$time_interval))

# GLM
glmm <- glmmTMB(post_count ~ turnpoint * owner_engagement, 
                data = npost_merged, family = nbinom2)
summary(glmm)
pR2(glmm)

# GAM
summary(gam(post_count ~ s(time, k=30) + owner_engagement, 
            data = npost_merged, family = nb(link = 'log'), method = 'ML'))
```

```{r modeling2_5, eval=FALSE, echo=FALSE, message=FALSE}
# Select Predictors
npost_intention <- npost_intention |>
  mutate(turnpoint = ifelse(time_interval < as.Date(turn_date), "before", "after"))
npost_intention$time <- as.numeric(npost_intention$time_interval - min(npost_intention$time_interval))

# GLM
glmm <- glmmTMB(post_count ~ turnpoint * quality * intention, 
                data = npost_intention, family = nbinom2)
summary(glmm)
pR2(glmm)

# GAM
summary(gam(post_count ~ s(time, k=30) + quality * intention, 
            data = npost_intention, family = nb(link = 'log'), method = 'ML'))
```


```{r modeling3, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Select Predictors
npost_tag <- npost_tag |>
  mutate(turnpoint = ifelse(time_interval < as.Date(turn_date), "before", "after"))
npost_tag$time <- as.numeric(npost_tag$time_interval - min(npost_tag$time_interval))

# GLMM
glmm <- glmmTMB(post_count ~ turnpoint + (1|tag), data = npost_tag, family = nbinom2)
summary(glmm)
pR2(glmm)

# GAMM
summary(gam(post_count ~ s(time, k=30) + s(tag, bs='re'), 
            data = npost_tag, family = nb(link = 'log'), method = 'ML'))
```


```{r modeling4, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Select Response Variables
user_ratio <- user_ratio |>
  mutate(experienced_share = experienced / (experienced + expert + normal),
         expert_share = expert / (experienced + expert + normal),
         normal_share = normal / (experienced + expert + normal),
         turnpoint = ifelse(time_interval < as.Date(turn_date), "before", "after"))
user_ratio$time <- as.numeric(user_ratio$time_interval - min(user_ratio$time_interval))
user_ratio$scaled_time <- scale(user_ratio$time)
user_ratio$response <- DR_data(user_ratio[, c("experienced_share", "expert_share", "normal_share")])

# Dirichlet Models
null <- DirichReg(response ~ 1, data = user_ratio)
summary(null)
mbd <- DirichReg(response ~ turnpoint + scaled_time, data = user_ratio)
summary(mbd)

# McFadden Pseudo-R2
anova_result <- anova(null, mbd)
anova_result
log_likelihood_fitted <- logLik(mbd)
log_likelihood_null <- logLik(null)
pseudo_r2_mcfadden <- 1 - (log_likelihood_fitted / log_likelihood_null)
cat("McFadden's pseudo-R2: ", pseudo_r2_mcfadden, "\n")
```


```{r modeling5, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
# Select Predictors
complexity_ratio <- posts_df |>
  mutate(time_interval = floor_date(creation_date, "day")) |>
  group_by(time_interval, complexity) |>
  summarise(count = n(), .groups = 'drop') |>
  pivot_wider(names_from = complexity, values_from = count, values_fill = list(count = 0)) |>
  mutate(ratio = `Complex`/ `Basic`,
         turnpoint = ifelse(time_interval < as.Date(turn_date), "before", "after"))
complexity_ratio$time <- as.numeric(
  complexity_ratio$time_interval - min(complexity_ratio$time_interval))
complexity_ratio$scaled_time <- scale(complexity_ratio$time)

# Beta
null <- betareg(ratio ~ 1, data = complexity_ratio)
summary(null)
beta <- betareg(ratio ~ turnpoint + scaled_time, data = complexity_ratio)
summary(beta)

# GAM
summary(gam(ratio ~ s(time, k=30) + turnpoint, data = complexity_ratio, 
            family = binomial(link = "logit"), method = 'ML'))
```


### 4 Summary

**4.1 Limitations**

While this study provides meaningful insights into the evolving landscape of Stack Overflow usage, several limitations should be considered. We also propose potential directions for improvement:

1. Manual Scoring for Classification
    
    The classification framework relied on manually defined scoring rules to assess post complexity and intention. Although machine learning models were introduced to mitigate bias, more advanced NLP methods such as zero-shot learning using transformer-based models or label-embedding approaches like Lbl2Vec could yield more accurate and less bias results.
    
2. Imbalance in AI-Relevant Content Detection
    
    Topic modeling via LDA was used to identify AI-relevant posts, but only two topics were categorized as such, resulting in a highly imbalanced dataset. Future work should consider collecting a larger volume of posts via the Stack Exchange API and applying more robust classification methods, allowing us to explore AI’s influence on content creation as well.
    
3. Lack of Direct AI Tool Usage Data
    
    This study inferred the impact of AI tools from trends in posting behavior, but it did not directly measure AI tool usage. To conclude a causal relationship, future research should incorporate data on individual users’ interactions with AI tools. Causal inference methods such as randomized controlled trials (RCTs) or quasi-experimental designs could then be employed for more rigorous evaluation.
    

By addressing these limitations, future studies can strengthen the validity and generalizability of findings, ultimately offering a clearer picture of how AI-assisted tools are transforming engagement and content on coding forums.

**4.2 Discussion**

Despite its limitations, this analysis offers valuable insights into how AI-assisted coding tools may be influencing engagement on platforms like Stack Overflow. The findings reveal a strong correlation between posting behavior and time, particularly around the release of ChatGPT in November 2022. Key shifts include a noticeable decline in basic question posts, a rising ratio of complex to basic posts, and evolving trends in tag-specific post counts. Interestingly, the decline in user proportion is most pronounced among expert users, while experienced and normal users have increased. These patterns suggest that beginner programmers may be turning to AI tools for quick solutions, reducing their reliance on forums for basic questions. Meanwhile, more advanced users might be using forums to engage in deeper, more complex discussions, potentially benefiting from AI assistance as a complement rather than a replacement. Overall, these trends point to the emergence of AI coding assistance tools, as a likely driving factor behind the observed changes in content and user behavior on coding forums.

### References

1. Wikipedia contributors. (2024, March 10). *ChatGPT*. Wikipedia. https://en.wikipedia.org/wiki/ChatGPT
2. Wikipedia contributors. (2024, March 10). *Stack Overflow*. Wikipedia. https://en.wikipedia.org/wiki/Stack_Overflow
3. Stack Exchange. (n.d.). *Stack Exchange API documentation*. Retrieved April 21, 2025, from https://api.stackexchange.com/docs
4. Stack Overflow Teams. (2024, March 10). *Content tags on Stack Overflow*. Retrieved April 21, 2025, from https://stackoverflowteams.help/en/articles/8872158-content-tags
5. StackApps. (n.d.). *StackApps OAuth registration*. Retrieved April 21, 2025, from https://stackapps.com/apps/oauth/register
6. Financial Times. (n.d.). *Visual vocabulary*. GitHub. Retrieved April 21, 2025, from https://github.com/Financial-Times/chart-doctor/tree/main/visual-vocabulary
7. Berinato, S. (n.d.). *Visual vocabulary* [Tableau dashboard]. Tableau Public. Retrieved April 21, 2025, from https://public.tableau.com/views/VisualVocabulary/VisualVocabulary?%3Aembed=y&%3Adisplay_count=yes&publish=yes&%3AshowVizHome=no
